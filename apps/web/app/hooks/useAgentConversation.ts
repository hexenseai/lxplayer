'use client';

import { useCallback, useEffect, useRef, useState } from 'react';
import { useVoiceStream } from 'voice-stream';
import type { ElevenLabsWebSocketEvent } from '../types/websocket';

const sendMessage = (websocket: WebSocket, request: object) => {
  if (websocket.readyState !== WebSocket.OPEN) {
    console.warn('‚ö†Ô∏è WebSocket is not open, cannot send message');
    return;
  }
  websocket.send(JSON.stringify(request));
};

export const useAgentConversation = () => {
  const websocketRef = useRef<WebSocket | null>(null);
  const [isConnected, setIsConnected] = useState<boolean>(false);
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [messages, setMessages] = useState<Array<{type: 'user' | 'agent', content: string}>>([]);
  const [error, setError] = useState<string | null>(null);
  const [isPlaying, setIsPlaying] = useState<boolean>(false);
  
  // Audio handling
  const audioQueueRef = useRef<string[]>([]);
  const isPlayingRef = useRef<boolean>(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const currentSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const isProcessingAudioRef = useRef<boolean>(false);
  
  // Audio chunk buffering for same event_id
  const audioChunkBufferRef = useRef<Map<number, string[]>>(new Map());
  const chunkTimeoutRef = useRef<Map<number, NodeJS.Timeout>>(new Map());

  // Stop any currently playing audio (but keep recording active)
  const stopCurrentAudio = useCallback(() => {
    console.log('üõë Stopping current audio playback (keeping recording active)...');
    
    // Stop HTML5 Audio
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current.currentTime = 0;
      currentAudioRef.current = null;
    }
    
    // Stop Web Audio API source
    if (currentSourceRef.current) {
      try {
        currentSourceRef.current.stop();
      } catch (error) {
        // Ignore error if already stopped
      }
      currentSourceRef.current = null;
    }
    
    setIsPlaying(false);
    isPlayingRef.current = false;
    isProcessingAudioRef.current = false;
    
    // Note: We don't stop recording here - continuous recording for natural conversation
    console.log('üé§ Recording continues during audio playback for natural conversation');
  }, []);

  // Process audio queue - sequential playback to prevent overlapping
  const processAudioQueue = useCallback(async () => {
    if (isProcessingAudioRef.current || audioQueueRef.current.length === 0) {
      return;
    }

    isProcessingAudioRef.current = true;
    console.log('üéµ Processing audio queue sequentially, items:', audioQueueRef.current.length);

    while (audioQueueRef.current.length > 0) {
      const audioBase64 = audioQueueRef.current.shift();
      if (audioBase64) {
        console.log('üîÑ Starting sequential audio playback...');
        
        try {
          // Play audio and wait for it to complete
          await playAudioWithWebAudio(audioBase64);
          
          // Wait for audio to finish before processing next
          while (isPlayingRef.current) {
            await new Promise(resolve => setTimeout(resolve, 100));
          }
          
          console.log('‚úÖ Audio finished, processing next in queue...');
        } catch (error) {
          console.error('‚ùå Error in sequential audio playback:', error);
        }
      }
    }

    isProcessingAudioRef.current = false;
    console.log('‚úÖ Audio queue processing completed (sequential)');
  }, []);

  // Add audio to queue instead of playing directly
  const queueAudio = useCallback((audioBase64: string) => {
    console.log('üì• Adding audio to queue, current queue size:', audioQueueRef.current.length);
    audioQueueRef.current.push(audioBase64);
    processAudioQueue();
  }, [processAudioQueue]);

  // Handle audio chunks - buffer chunks with same event_id
  const handleAudioChunk = useCallback((eventId: number, audioBase64: string) => {
    console.log(`üì• Received audio chunk for event_id: ${eventId}, size: ${audioBase64.length} chars`);
    
    // Get or create buffer for this event_id
    if (!audioChunkBufferRef.current.has(eventId)) {
      audioChunkBufferRef.current.set(eventId, []);
      console.log(`üÜï Created new buffer for event_id: ${eventId}`);
    } else {
      console.log(`üîÑ Reusing existing buffer for event_id: ${eventId}`);
    }
    
    // Add chunk to buffer
    const buffer = audioChunkBufferRef.current.get(eventId)!;
    buffer.push(audioBase64);
    console.log(`üì¶ Added chunk to buffer, total chunks for event_id ${eventId}: ${buffer.length}`);
    
    // Clear existing timeout for this event_id
    const existingTimeout = chunkTimeoutRef.current.get(eventId);
    if (existingTimeout) {
      clearTimeout(existingTimeout);
      console.log(`‚è∞ Cleared existing timeout for event_id: ${eventId}`);
    }
    
    // Set timeout to process buffered chunks (wait for more chunks)
    const timeout = setTimeout(() => {
      const chunks = audioChunkBufferRef.current.get(eventId);
      if (chunks && chunks.length > 0) {
        console.log(`üîó Combining ${chunks.length} chunks for event_id: ${eventId}`);
        
        // Properly combine audio chunks by decoding, concatenating binary data, then re-encoding
        try {
          console.log(`üîß Properly combining ${chunks.length} audio chunks...`);
          
          // Decode each base64 chunk to binary data
          const binaryChunks: Uint8Array[] = [];
          let totalLength = 0;
          
          for (const chunk of chunks) {
            try {
              const binaryString = atob(chunk);
              const uint8Array = new Uint8Array(binaryString.length);
              for (let i = 0; i < binaryString.length; i++) {
                uint8Array[i] = binaryString.charCodeAt(i);
              }
              binaryChunks.push(uint8Array);
              totalLength += uint8Array.length;
              console.log(`‚úÖ Decoded chunk: ${uint8Array.length} bytes`);
            } catch (decodeError) {
              console.error(`‚ùå Failed to decode chunk:`, decodeError);
              continue; // Skip invalid chunks
            }
          }
          
          if (binaryChunks.length === 0) {
            console.error(`‚ùå No valid chunks to combine for event_id ${eventId}`);
            return;
          }
          
          // Concatenate all binary data
          const combinedBinary = new Uint8Array(totalLength);
          let offset = 0;
          
          for (const chunk of binaryChunks) {
            combinedBinary.set(chunk, offset);
            offset += chunk.length;
          }
          
          // Convert back to base64
          let binaryString = '';
          for (let i = 0; i < combinedBinary.length; i++) {
            binaryString += String.fromCharCode(combinedBinary[i]);
          }
          const combinedAudio = btoa(binaryString);
          
          console.log(`‚úÖ Combined audio: ${combinedBinary.length} bytes ‚Üí ${combinedAudio.length} chars base64`);
          
          // Queue the combined audio
          queueAudio(combinedAudio);
          
        } catch (combineError) {
          console.error(`‚ùå Error combining chunks for event_id ${eventId}:`, combineError);
          // Try to queue individual chunks as fallback
          console.log(`üîÑ Fallback: Queuing individual chunks for event_id ${eventId}`);
          for (const chunk of chunks) {
            try {
              // Validate chunk before queuing
              atob(chunk); // Test decode
              queueAudio(chunk);
              console.log(`‚úÖ Queued individual chunk: ${chunk.length} chars`);
            } catch (chunkError) {
              console.error(`‚ùå Skipping invalid chunk:`, chunkError);
            }
          }
        }
        
        // Clean up ONLY after processing
        audioChunkBufferRef.current.delete(eventId);
        chunkTimeoutRef.current.delete(eventId);
        console.log(`üßπ Cleaned up buffers for event_id: ${eventId}`);
      }
    }, 250); // Optimized: Fast enough for real-time, slow enough to collect chunks
    
    chunkTimeoutRef.current.set(eventId, timeout);
  }, [queueAudio]);

  // Use voice-stream for microphone handling
  const { startStreaming, stopStreaming } = useVoiceStream({
    onAudioChunked: (audioData) => {
      if (!websocketRef.current) return;
      console.log('üé§ Sending audio chunk via voice-stream:', audioData.length, 'chars');
      sendMessage(websocketRef.current, {
        user_audio_chunk: audioData,
      });
    },
  });

  const playAudioWithHTML5 = useCallback(async (audioBase64: string) => {
    try {
      console.log('üéµ Playing audio with HTML5 Audio API...');
      
      if (!audioBase64 || audioBase64.length === 0) {
        console.error('‚ùå Empty audio data received');
        return;
      }
      
      // Stop any currently playing audio first
      stopCurrentAudio();
      
      // Minimal wait to ensure previous audio is stopped  
      await new Promise(resolve => setTimeout(resolve, 10));
      
      // Create audio blob from base64
      const binaryString = atob(audioBase64);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      // Try different audio formats
      const formats = [
        'audio/wav',
        'audio/mpeg',
        'audio/mp3',
        'audio/ogg',
        'audio/webm'
      ];
      
      for (const format of formats) {
        try {
          const blob = new Blob([bytes], { type: format });
          const audioUrl = URL.createObjectURL(blob);
          const audio = new Audio(audioUrl);
          
          // Optimize playback rate for natural conversation speed
          audio.playbackRate = 1.0; // Normal speed for better sync
          
          audio.onloadeddata = () => {
            console.log(`‚úÖ Audio loaded successfully with format: ${format}`);
            console.log(`üîç Audio duration: ${audio.duration}s, playback rate: ${audio.playbackRate}`);
          };
          
          audio.onended = () => {
            console.log('üéµ HTML5 audio playback ended');
            setIsPlaying(false);
            isPlayingRef.current = false;
            currentAudioRef.current = null;
            URL.revokeObjectURL(audioUrl);
          };
          
          audio.onerror = (error) => {
            console.error(`‚ùå HTML5 audio error with ${format}:`, error);
            URL.revokeObjectURL(audioUrl);
          };
          
          // Store reference to current audio
          currentAudioRef.current = audio;
          
          await audio.play();
          setIsPlaying(true);
          isPlayingRef.current = true;
          console.log(`‚úÖ HTML5 audio playback started with ${format} at ${audio.playbackRate}x speed`);
          return; // Success, exit the loop
          
        } catch (formatError) {
          console.log(`‚è≠Ô∏è Format ${format} failed, trying next...`);
          continue;
        }
      }
      
      // If HTML5 fails, fallback to Web Audio API
      console.log('üîÑ HTML5 Audio failed, falling back to Web Audio API...');
      return playAudioWithWebAudio(audioBase64);
      
    } catch (error) {
      console.error('‚ùå Error with HTML5 audio, falling back to Web Audio API:', error);
      return playAudioWithWebAudio(audioBase64);
    }
  }, [stopCurrentAudio]);

  const playAudioWithWebAudio = useCallback(async (audioBase64: string) => {
    try {
      console.log('üéµ Playing audio with Web Audio API (async mode)...');
      
      // Check if audio data is empty
      if (!audioBase64 || audioBase64.length === 0) {
        console.error('‚ùå Empty audio data received');
        return;
      }
      
      // Don't stop current audio - allow parallel playback for natural conversation
      // Multiple audio sources can play simultaneously
      console.log('üîÑ Starting parallel audio playback...');
      
      // Convert base64 to ArrayBuffer with validation
      let binaryString;
      try {
        binaryString = atob(audioBase64);
      } catch (atobError) {
        console.error('‚ùå Invalid base64 audio data:', atobError);
        console.log('üîç Base64 sample:', audioBase64.substring(0, 50) + '...');
        return;
      }
      
      const arrayBuffer = new ArrayBuffer(binaryString.length);
      const uint8Array = new Uint8Array(arrayBuffer);
      
      for (let i = 0; i < binaryString.length; i++) {
        uint8Array[i] = binaryString.charCodeAt(i);
      }
      
      console.log('üîç Audio buffer size:', arrayBuffer.byteLength, 'bytes');

      // Create AudioContext if not exists
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      }

      const audioContext = audioContextRef.current;

      // Clone the ArrayBuffer before decodeAudioData (it consumes the buffer)
      const clonedBuffer = arrayBuffer.slice(0);
      
      // Try to decode as standard audio format first
      try {
        console.log('üîÑ Attempting standard audio decode...');
        const decodedAudio = await audioContext.decodeAudioData(arrayBuffer);
        
        const source = audioContext.createBufferSource();
        source.buffer = decodedAudio;
        source.connect(audioContext.destination);
        
        // Single source tracking for sequential playback
        currentSourceRef.current = source;
        
        source.onended = () => {
          console.log('üéµ Web Audio API playback ended (sequential)');
          setIsPlaying(false);
          isPlayingRef.current = false;
          currentSourceRef.current = null;
        };
        
        source.start();
        setIsPlaying(true);
        isPlayingRef.current = true;
        console.log('‚úÖ Standard audio played successfully (async)');
        
      } catch (decodeError) {
        console.log('üîÑ Standard decode failed, trying multiple sample rates...');
        console.log('üîç Decode error:', decodeError);
        
        // Try different sample rates commonly used by ElevenLabs
        // ElevenLabs typically uses 22050Hz for WebSocket streaming
        const sampleRates = [16000, 22050, 24000, 44100, 48000];
        
        for (const sampleRate of sampleRates) {
          try {
            console.log(`üîÑ Trying sample rate: ${sampleRate}Hz`);
            
            // Handle as raw PCM data (16-bit, mono)
            const numberOfChannels = 1;
            const length = Math.floor(clonedBuffer.byteLength / 2); // 16-bit samples
            
            console.log('üîç PCM Processing Details:', {
              sampleRate,
              originalBufferSize: arrayBuffer.byteLength,
              clonedBufferSize: clonedBuffer.byteLength,
              calculatedLength: length,
              expectedDuration: length / sampleRate
            });
            
            // Check if we have valid audio data
            if (length <= 0) {
              console.error('‚ùå Invalid audio data: length is 0 or negative');
              continue;
            }
            
            // Skip very small audio chunks
            if (length < 50) {
              console.log('‚è≠Ô∏è Skipping very small audio chunk:', length, 'samples');
              continue;
            }
            
            const audioBuffer = audioContext.createBuffer(numberOfChannels, length, sampleRate);
            const channelData = audioBuffer.getChannelData(0);
            const dataView = new DataView(clonedBuffer);
            
            // Convert 16-bit PCM to float32
            for (let i = 0; i < length; i++) {
              const byteOffset = i * 2;
              if (byteOffset + 1 < clonedBuffer.byteLength) {
                const sample = dataView.getInt16(byteOffset, true); // Little-endian
                channelData[i] = sample / 32768.0; // Convert to float32 range [-1, 1]
              } else {
                break;
              }
            }
            
            console.log('‚úÖ PCM conversion completed, samples processed:', length);
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            // Single source tracking for sequential playback
            currentSourceRef.current = source;
            
            source.onended = () => {
              console.log('üéµ PCM audio playback ended (sequential)');
              setIsPlaying(false);
              isPlayingRef.current = false;
              currentSourceRef.current = null;
              console.log('üîá Audio source finished');
            };
            
            source.onerror = (error) => {
              console.error('‚ùå Audio source error:', error);
              setIsPlaying(false);
              isPlayingRef.current = false;
              currentSourceRef.current = null;
            };
            
            source.start();
            setIsPlaying(true);
            isPlayingRef.current = true;
            
            console.log(`‚úÖ Raw PCM audio playback started successfully with ${sampleRate}Hz!`, {
              duration: audioBuffer.duration.toFixed(3) + 's',
              sampleRate: audioBuffer.sampleRate,
              channels: audioBuffer.numberOfChannels
            });
            
            return; // Success, exit the loop
            
          } catch (pcmError) {
            console.error(`‚ùå PCM processing error with ${sampleRate}Hz:`, pcmError);
            continue;
          }
        }
        
        console.error('‚ùå All sample rates failed');
        setIsPlaying(false);
        isPlayingRef.current = false;
      }
      
    } catch (error) {
      console.error('‚ùå Error playing audio:', error);
      setIsPlaying(false);
      isPlayingRef.current = false;
    }
  }, [stopCurrentAudio]);

  // Alternative: ElevenLabs React library method (if needed)
  const playAudioWithElevenLabsReact = useCallback(async (audioBase64: string) => {
    try {
      console.log('üéµ Attempting ElevenLabs React library method...');
      
      // This would use @elevenlabs/react library
      // Currently using manual implementation for better control
      // To use ElevenLabs React library, you would:
      // 1. Import { ElevenLabsConversation } from '@elevenlabs/react'
      // 2. Use their built-in audio handling
      
      console.log('‚ö†Ô∏è ElevenLabs React library method not implemented yet');
      console.log('üîÑ Falling back to HTML5 Audio...');
      
      return playAudioWithHTML5(audioBase64);
      
    } catch (error) {
      console.error('‚ùå ElevenLabs React library error:', error);
      return playAudioWithHTML5(audioBase64);
    }
  }, [playAudioWithHTML5]);

  const playAudio = useCallback(async (audioBase64: string) => {
    // Direct queue for manual audio (not chunked)
    queueAudio(audioBase64);
  }, [queueAudio]);

  const startConversation = useCallback(async (agentId: string) => {
    if (isConnected) return;
    
    try {
      console.log('üöÄ Starting ElevenLabs WebSocket conversation with agent:', agentId);
      setError(null);
      setMessages([]);
      
      // Stop any currently playing audio
      stopCurrentAudio();
      
      // Create WebSocket connection
      const websocket = new WebSocket(`wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${agentId}`);
      
      websocket.onopen = () => {
        console.log('‚úÖ ElevenLabs WebSocket connected');
        setIsConnected(true);
        
        // Small delay to ensure state is updated
        setTimeout(() => {
          console.log('üîÑ WebSocket connection established, ready for audio streaming');
        }, 100);
      };
      
      websocket.onmessage = async (event) => {
        const data = JSON.parse(event.data) as ElevenLabsWebSocketEvent;
        
        // Handle ping events to keep connection alive
        if (data.type === "ping") {
          setTimeout(() => {
            sendMessage(websocket, {
              type: "pong",
              event_id: data.ping_event.event_id,
            });
          }, data.ping_event.ping_ms);
        }
        
        if (data.type === "user_transcript") {
          const { user_transcription_event } = data;
          console.log("üìù User transcript:", user_transcription_event.user_transcript);
          // Voice-only mode: Skip message storage for better performance
        }
        
        if (data.type === "agent_response") {
          const { agent_response_event } = data;
          console.log("ü§ñ Agent response:", agent_response_event.agent_response);
          // Voice-only mode: Skip message storage for better performance
        }
        
        if (data.type === "agent_response_correction") {
          const { agent_response_correction_event } = data;
          console.log("üîÑ Agent response correction:", agent_response_correction_event.corrected_agent_response);
          // Voice-only mode: Skip message storage for better performance
        }
        
        if (data.type === "interruption") {
          console.log("‚ö†Ô∏è Interruption:", data.interruption_event.reason);
        }
        
        if (data.type === "audio") {
          const { audio_event } = data;
          console.log("üéµ Audio event received, event_id:", audio_event.event_id);
          // Handle audio chunk with buffering for same event_id
          handleAudioChunk(audio_event.event_id, audio_event.audio_base_64);
        }
      };
      
      websocketRef.current = websocket;
      
      websocket.onclose = async () => {
        console.log('üîå ElevenLabs WebSocket disconnected');
        websocketRef.current = null;
        setIsConnected(false);
        setIsRecording(false);
        stopStreaming();
      };
      
      websocket.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        setError('WebSocket connection error');
      };
      
    } catch (error) {
      console.error('‚ùå Error starting conversation:', error);
      setError('Failed to start conversation');
    }
  }, [isConnected, playAudio, stopStreaming, stopCurrentAudio, handleAudioChunk]);

  const stopConversation = useCallback(async () => {
    if (!websocketRef.current) return;
    
    console.log('üõë Stopping ElevenLabs conversation...');
    
    // Stop any currently playing audio and clear queue
    stopCurrentAudio();
    audioQueueRef.current = [];
    
    // Stop current audio source if playing
    if (currentSourceRef.current) {
      try {
        currentSourceRef.current.stop();
      } catch (error) {
        // Ignore if already stopped
      }
      currentSourceRef.current = null;
    }
    
    // Clear audio chunk buffers and timeouts
    chunkTimeoutRef.current.forEach((timeout) => clearTimeout(timeout));
    chunkTimeoutRef.current.clear();
    audioChunkBufferRef.current.clear();
    
    console.log('üßπ Audio queue, current source and chunk buffers cleared');
    
    websocketRef.current.close();
    setIsRecording(false);
    stopStreaming();
  }, [stopStreaming, stopCurrentAudio]);

  const toggleRecording = useCallback(async () => {
    if (!isConnected) {
      console.error('‚ùå Cannot start recording: WebSocket not connected');
      return;
    }
    
    if (isRecording) {
      console.log('üõë Stopping audio recording...');
      stopStreaming();
      setIsRecording(false);
    } else {
      console.log('üé§ Starting audio recording...');
      try {
        await startStreaming();
        setIsRecording(true);
      } catch (error) {
        console.error('‚ùå Error starting recording:', error);
        setError('Failed to start recording');
      }
    }
  }, [isConnected, isRecording, startStreaming, stopStreaming]);

  const sendContextualUpdate = useCallback((text: string) => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      console.warn('‚ö†Ô∏è Cannot send contextual update: WebSocket not connected');
      return;
    }
    
    console.log('üìù Sending contextual update:', text);
    sendMessage(websocketRef.current, {
      type: "contextual_update",
      text: text
    });
  }, []);

  useEffect(() => {
    return () => {
      if (websocketRef.current) {
        websocketRef.current.close();
      }
    };
  }, []);

  return {
    startConversation,
    stopConversation,
    toggleRecording,
    sendContextualUpdate,
    isConnected,
    isRecording,
    isPlaying,
    messages,
    error
  };
};